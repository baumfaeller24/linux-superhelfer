qwen3-coder:30b
/
model
1194192cf2a1 · 19GB
Metadata
general.architecture
qwen3moe
general.file_type
Q4_K_M
qwen3moe.attention.head_count
32
qwen3moe.attention.head_count_kv
4
qwen3moe.attention.key_length
128
qwen3moe.attention.layer_norm_rms_epsilon
1e-06
qwen3moe.attention.value_length
128
qwen3moe.block_count
48
qwen3moe.context_length
262144
qwen3moe.embedding_length
2048
qwen3moe.expert_count
128
qwen3moe.expert_feed_forward_length
768
qwen3moe.expert_shared_feed_forward_length
0
qwen3moe.expert_used_count
8
qwen3moe.feed_forward_length
5472
qwen3moe.rope.freq_base
1e+07
tokenizer.ggml.add_bos_token
false
tokenizer.ggml.eos_token_id
151645
tokenizer.ggml.merges
[Ġ Ġ, ĠĠ ĠĠ, i n, Ġ t, ĠĠĠĠ ĠĠĠĠ, ...]
tokenizer.ggml.model
gpt2
tokenizer.ggml.padding_token_id
151643
tokenizer.ggml.pre
qwen2
tokenizer.ggml.token_type
[1, 1, 1, 1, 1, ...]
tokenizer.ggml.tokens
[!, ", #, $, %, ...]
Tensor
Name
Type
Shape
token_embd.weight
Q4_K
[2048, 151936]
